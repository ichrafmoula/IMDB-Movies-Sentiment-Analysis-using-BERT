{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_imdb_lstm.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1PeE7RCHcNw5166HYodLKvLeeq2_ypjPj",
      "authorship_tag": "ABX9TyOA8XfrNgynp2sXqSTAk8+d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ichrafmoula/IMDB-Movies-Sentiment-Analysis-using-BERT/blob/master/sentiment_analysis_imdb_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSTllfmHTeGw"
      },
      "source": [
        "**Import all **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfJcG712EcyC"
      },
      "source": [
        "import pandas as pd    # to load dataset\n",
        "import numpy as np     # for mathematic equation\n",
        "from nltk.corpus import stopwords   # to get collection of stopwords\n",
        "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
        "from tensorflow.keras.models import Sequential     # the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
        "from tensorflow.keras.models import load_model   # load saved model\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jglu0_ELUg3g"
      },
      "source": [
        "**Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H3PxbS6xknj"
      },
      "source": [
        "#load the Google Play app reviews dataset\n",
        "!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
        "!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7qz-wbqxmVU"
      },
      "source": [
        "data = pd.read_csv(\"reviews.csv\",engine='python' ,encoding='utf-8' ,error_bad_lines=False)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8PCMq5YEt3B"
      },
      "source": [
        "#Preview dataset\n",
        "\n",
        "data = pd.read_csv('/content/IMDB Dataset.csv' , engine='python' ,encoding='utf-8' ,error_bad_lines=False)\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7djo36idJPt"
      },
      "source": [
        "#the distribution of positive and negative sentiments in our dataset\n",
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(x='sentiment', data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FSUVSPzdnbH"
      },
      "source": [
        "# Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifsuAZ6-dX4S"
      },
      "source": [
        "#Declaring the english stop words\n",
        "english_stops = set(stopwords.words('english'))\n",
        "english_stops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-PvAu9jflSG"
      },
      "source": [
        "def load_dataset(df):\n",
        "    x_data = df['review']       # Reviews/Input\n",
        "    y_data = df['sentiment']    # Sentiment/Output\n",
        "\n",
        "    # PRE-PROCESS REVIEW\n",
        "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
        "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
        "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
        "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
        "    \n",
        "    # ENCODE SENTIMENT -> 0 & 1\n",
        "    y_data = y_data.replace('positive', 1)\n",
        "    y_data = y_data.replace('negative', 0)\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "x_data, y_data = load_dataset(data)\n",
        "print(x_data, '\\n')\n",
        "print('Sentiment')\n",
        "print(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjuUcilqgHXu"
      },
      "source": [
        "\n",
        "# **Split Dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qve6QD0qCoRs"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TPj1_mCgEFl"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
        "\n",
        "print('Train Set')\n",
        "print(x_train, '\\n')\n",
        "print(x_test, '\\n')\n",
        "print('Test Set')\n",
        "print(y_train, '\\n')\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtWifPh-nLHa"
      },
      "source": [
        "#Function for getting the maximum review length, by calculating the mean of all the reviews length\n",
        "def get_max_length():\n",
        "    review_length = []\n",
        "    for review in x_train:\n",
        "        review_length.append(len(review))\n",
        "\n",
        "    return int(np.ceil(np.mean(review_length)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RsjPce3nVAv"
      },
      "source": [
        "# Tokenize and Pad/Truncate Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6idR1tznRjN"
      },
      "source": [
        "# ENCODE REVIEW\n",
        "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
        "token.fit_on_texts(x_train)\n",
        "x_train = token.texts_to_sequences(x_train)\n",
        "x_test = token.texts_to_sequences(x_test)\n",
        "\n",
        "max_length = get_max_length()\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
        "\n",
        "print('Encoded X Train\\n', x_train, '\\n')\n",
        "print('Encoded X Test\\n', x_test, '\\n')\n",
        "print('Maximum review length: ', max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou_PNaKsaqmc"
      },
      "source": [
        "# Build Architecture/Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq3clDkTauJM"
      },
      "source": [
        "# ARCHITECTURE\n",
        "from tensorflow.keras.models import Sequential     # the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense \n",
        "EMBED_DIM = 32\n",
        "lstm_out = 64\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
        "model.add(LSTM(lstm_out))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy', metrics.Recall(),metrics.Precision()]) \n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "gMiJN6BR5EUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIntTHZTa9Za"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x53Z2vx8a7ih"
      },
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    'models/LSTM.h5',\n",
        "    monitor='accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLbJwMVdbP7F"
      },
      "source": [
        "history = model.fit(x_train ,\n",
        "                    y_train, \n",
        "                    validation_data=(x_test, y_test), \n",
        "                    epochs=10, \n",
        "                    batch_size=32, \n",
        "                    callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "socres = model.evaluate(x_test , y_test , verbose=0)\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (socres[1]*100))"
      ],
      "metadata": {
        "id": "kQHQb2ARh2m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accr = model.evaluate(x_train, y_train)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.4f}'.format(accr[0],accr[1]))"
      ],
      "metadata": {
        "id": "BP3JGch8Km1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.title('Model Loss')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "HIwsY2bTKh-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sCFn37-wKvVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['recall_5'])\n",
        "plt.plot(history.history['val_recall_5'])\n",
        "plt.title('Model Recall')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y9IZvmIIuc7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['precision_5'])\n",
        "plt.plot(history.history['val_precision_5'])\n",
        "plt.title('Model Precision')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AsRZPBYSu7NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_nn_train = model.predict(x_train)\n",
        "predictions_nn_test = model.predict(x_test)\n",
        "for i in range(len(predictions_nn_train)):\n",
        "    if predictions_nn_train[i][0] < 0.5:\n",
        "        predictions_nn_train[i][0] = 0\n",
        "    else:\n",
        "        predictions_nn_train[i][0] = 1\n",
        "        \n",
        "for i in range(len(predictions_nn_test)):\n",
        "    if predictions_nn_test[i][0] < 0.5:\n",
        "        predictions_nn_test[i][0] = 0\n",
        "    else:\n",
        "        predictions_nn_test[i][0] = 1\n",
        "print('Train accuracy:', accuracy_score(y_train, predictions_nn_train))\n",
        "print('Test accuracy', accuracy_score(y_test, predictions_nn_test))"
      ],
      "metadata": {
        "id": "2jiCGewHt0Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "test_acc = model.evaluate(x_test , y_test, verbose=0)"
      ],
      "metadata": {
        "id": "Y7QtnEgTiMxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "# plot accuracy during training\n",
        "\n",
        "print('')\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "qaUdKDKQiTh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "x = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x, acc, 'b', label='Training acc')\n",
        "plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x, loss, 'b', label='Training loss')\n",
        "plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "7kMr74oYjkxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEST BILSTM**"
      ],
      "metadata": {
        "id": "TwXDyX9rZ0-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Embedding(total_words, 32, input_length = max_length))\n",
        "model2.add(Bidirectional(LSTM(64)))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', metrics.Recall(),metrics.Precision()]) "
      ],
      "metadata": {
        "id": "L0OvCuS7WEnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2=model2.fit(x_train, y_train,\n",
        "           batch_size=32,\n",
        "           epochs=12,\n",
        "           validation_data=[x_test, y_test])\n",
        "print(history2.history['loss'])\n",
        "print(history2.history['accuracy']) "
      ],
      "metadata": {
        "id": "MJfLs52DWmBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.title('Model Loss')\n",
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history2.history['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "T88MWdIIW4S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history2.history['accuracy'])\n",
        "plt.plot(history2.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0VX0O2aBW8QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history2.history['recall_6'])\n",
        "plt.plot(history2.history['val_recall_6'])\n",
        "plt.title('Model Recall')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fLYqPt24ZaEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history2.history['precision_6'])\n",
        "plt.plot(history2.history['val_precision_6'])\n",
        "plt.title('Model Precision')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "IU1urEasYZ-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_nn_train = model2.predict(x_train)\n",
        "predictions_nn_test = model2.predict(x_test)\n",
        "for i in range(len(predictions_nn_train)):\n",
        "    if predictions_nn_train[i][0] < 0.5:\n",
        "        predictions_nn_train[i][0] = 0\n",
        "    else:\n",
        "        predictions_nn_train[i][0] = 1\n",
        "        \n",
        "for i in range(len(predictions_nn_test)):\n",
        "    if predictions_nn_test[i][0] < 0.5:\n",
        "        predictions_nn_test[i][0] = 0\n",
        "    else:\n",
        "        predictions_nn_test[i][0] = 1\n",
        "print('Train accuracy:', accuracy_score(y_train, predictions_nn_train))\n",
        "print('Test accuracy', accuracy_score(y_test, predictions_nn_test))"
      ],
      "metadata": {
        "id": "YBdFgfcZaqN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qmqy-aHbUO0"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJTHHUFPbT6D"
      },
      "source": [
        "y_pred = model.predict_classes(x_test, batch_size = 128)\n",
        "\n",
        "true = 0\n",
        "for i, y in enumerate(y_test):\n",
        "    if y == y_pred[i]:\n",
        "        true += 1\n",
        "\n",
        "print('Correct Prediction: {}'.format(true))\n",
        "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
        "print('Accuracy: {}'.format(true/len(y_pred)*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBRdcOprbpQ8"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ww7hk1QboBJ"
      },
      "source": [
        "loaded_model =load_model('/content/sample_data/models/LSTM.h5')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZa7iQJ1b2GO"
      },
      "source": [
        "review = str(input('Movie Review: '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp7tvX9eb_sz"
      },
      "source": [
        "# Pre-process input\n",
        "regex = re.compile(r'[^a-zA-Z\\s]')\n",
        "review = regex.sub('', review)\n",
        "print('Cleaned: ', review)\n",
        "\n",
        "words = review.split(' ')\n",
        "filtered = [w for w in words if w not in english_stops]\n",
        "filtered = ' '.join(filtered)\n",
        "filtered = [filtered.lower()]\n",
        "\n",
        "print('Filtered: ', filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtuQipyUcAot"
      },
      "source": [
        "tokenize_words = token.texts_to_sequences(filtered)\n",
        "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
        "print(tokenize_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRWptkuXfXfM"
      },
      "source": [
        "result = loaded_model.predict(tokenize_words)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVtIYc0rcT_U"
      },
      "source": [
        "if result >= 0.7:\n",
        "    print('positive')\n",
        "else:\n",
        "    print('negative')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}